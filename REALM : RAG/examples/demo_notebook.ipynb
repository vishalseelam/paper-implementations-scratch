{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# REALM/RAG Implementation Demo\n",
        "\n",
        "This notebook demonstrates the comprehensive REALM/RAG implementation based on:\n",
        "- **REALM**: Retrieval-Augmented Language Model Pre-training (Guu et al., 2020)\n",
        "- **RAG**: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (Lewis et al., 2020)\n",
        "\n",
        "## Features\n",
        "- Dense passage retrieval with FAISS indexing\n",
        "- Seq2seq generation with retrieved context\n",
        "- Joint training of retrieval and generation\n",
        "- Comprehensive evaluation metrics\n",
        "- Support for both RAG and REALM architectures\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Setup and Installation\n",
        "\n",
        "First, let's install the required dependencies and set up the environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required modules\n",
        "import sys\n",
        "import os\n",
        "import logging\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# Add src to path\n",
        "sys.path.append(os.path.join(os.getcwd(), '..', 'src'))\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"Setup complete!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Configuration and Model Initialization\n",
        "\n",
        "Let's set up the configuration and initialize the REALM/RAG model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from config import REALMRAGConfig, DEFAULT_CONFIG\n",
        "from realm_rag import REALMRAGModel\n",
        "from data_utils import create_sample_data\n",
        "\n",
        "# Create configuration\n",
        "config = DEFAULT_CONFIG\n",
        "\n",
        "# Modify for demo (smaller models and data)\n",
        "config.model.num_retrieved_docs = 3\n",
        "config.training.batch_size = 2\n",
        "config.training.num_epochs = 1\n",
        "config.experiment.use_wandb = False\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"  Model type: RAG\")\n",
        "print(f\"  Retrieved docs: {config.model.num_retrieved_docs}\")\n",
        "print(f\"  Batch size: {config.training.batch_size}\")\n",
        "print(f\"  Generator model: {config.model.generator_model_name}\")\n",
        "\n",
        "# Create sample data\n",
        "sample_data = create_sample_data(num_examples=20, num_docs=50)\n",
        "\n",
        "print(\"\\nSample data created:\")\n",
        "print(f\"  Training examples: {len(sample_data['datasets']['train'])}\")\n",
        "print(f\"  Test examples: {len(sample_data['datasets']['test'])}\")\n",
        "print(f\"  Knowledge base size: {len(sample_data['knowledge_base'])}\")\n",
        "\n",
        "# Initialize model\n",
        "model = REALMRAGModel(config, model_type=\"rag\")\n",
        "print(\"\\nModel initialized successfully!\")\n",
        "\n",
        "# Prepare knowledge base\n",
        "model.prepare_knowledge_base(sample_data['knowledge_base'])\n",
        "print(\"Knowledge base prepared!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Testing Question Answering\n",
        "\n",
        "Let's test the model's ability to answer questions using retrieval-augmented generation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test questions\n",
        "test_questions = [\n",
        "    \"What is the capital of Country0?\",\n",
        "    \"How does Process1 work?\",\n",
        "    \"What is Concept2?\",\n",
        "    \"Who invented Invention3?\"\n",
        "]\n",
        "\n",
        "print(\"Testing inference with sample questions...\")\n",
        "results = model.retrieve_and_generate(\n",
        "    test_questions,\n",
        "    k=3,\n",
        "    return_retrieved_docs=True\n",
        ")\n",
        "\n",
        "print(\"\\n=== Question Answering Results ===\")\n",
        "for i, question in enumerate(test_questions):\n",
        "    print(f\"\\nQuestion {i+1}: {question}\")\n",
        "    print(f\"Answer: {results['answers'][i]}\")\n",
        "    print(f\"Retrieved {len(results['retrieved_docs'][i])} documents\")\n",
        "    \n",
        "    # Show top retrieved document\n",
        "    top_doc = results['retrieved_docs'][i][0]\n",
        "    print(f\"Top document (score: {top_doc['score']:.4f}):\")\n",
        "    print(f\"  {top_doc['text'][:100]}...\")\n",
        "\n",
        "print(\"\\nâœ… Question answering test completed!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Model Evaluation\n",
        "\n",
        "Let's evaluate the model using comprehensive metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from evaluator import REALMRAGEvaluator\n",
        "from data_utils import REALMRAGDataset\n",
        "\n",
        "# Create evaluator\n",
        "evaluator = REALMRAGEvaluator(model, config)\n",
        "\n",
        "# Create test dataset\n",
        "test_dataset = REALMRAGDataset(sample_data['datasets']['test'], config.model)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Running evaluation...\")\n",
        "eval_results = evaluator.evaluate_dataset(test_dataset, split=\"test\")\n",
        "\n",
        "print(\"\\n=== Evaluation Results ===\")\n",
        "print(f\"Exact Match: {eval_results['exact_match']:.4f}\")\n",
        "print(f\"F1 Score: {eval_results['f1_score']:.4f}\")\n",
        "print(f\"Precision: {eval_results['precision']:.4f}\")\n",
        "print(f\"Recall: {eval_results['recall']:.4f}\")\n",
        "print(f\"BLEU-4: {eval_results['bleu_4']:.4f}\")\n",
        "print(f\"ROUGE-L F1: {eval_results['rougeL_f1']:.4f}\")\n",
        "print(f\"Retrieval F1: {eval_results['retrieval_f1']:.4f}\")\n",
        "print(f\"Mean Reciprocal Rank: {eval_results['mean_reciprocal_rank']:.4f}\")\n",
        "print(f\"Hit@1: {eval_results['hit_at_1']:.4f}\")\n",
        "print(f\"Hit@3: {eval_results['hit_at_3']:.4f}\")\n",
        "\n",
        "print(\"\\nðŸŽ‰ REALM/RAG Demo completed successfully!\")\n",
        "print(\"\\nKey features demonstrated:\")\n",
        "print(\"âœ“ Dense passage retrieval with FAISS\")\n",
        "print(\"âœ“ Seq2seq generation with retrieved context\")\n",
        "print(\"âœ“ Comprehensive evaluation metrics\")\n",
        "print(\"âœ“ Both RAG and REALM architectures support\")\n",
        "print(\"âœ“ End-to-end question answering pipeline\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
