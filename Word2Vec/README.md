# Word2Vec: Complete Implementation from Scratch

A comprehensive, production-ready implementation of the Word2Vec algorithm based on the original paper by Mikolov et al. This implementation includes both CBOW and Skip-gram architectures with hierarchical softmax and negative sampling training methods.

## üöÄ Features

### Core Models
- **Skip-gram**: Predicts context words from target words
- **CBOW (Continuous Bag of Words)**: Predicts target words from context words

### Training Methods
- **Hierarchical Softmax**: Efficient training using Huffman trees
- **Negative Sampling**: Alternative training method with subsampling

### Advanced Features
- **Subsampling**: Reduces frequent word occurrences
- **Dynamic Context Windows**: Variable window sizes during training
- **Learning Rate Scheduling**: Adaptive learning rate decay
- **Batch Processing**: Efficient GPU/CPU batch training
- **Model Checkpointing**: Save/resume training progress

### Evaluation & Analysis
- **Word Similarity**: Cosine similarity between word embeddings
- **Analogy Tasks**: Solve word analogies (king - man + woman = queen)
- **Clustering Analysis**: Group semantically similar words
- **Visualization**: t-SNE and PCA embedding visualizations
- **Comprehensive Evaluation**: Standard NLP evaluation metrics

## üìÅ Project Structure

```
Word2Vec/
‚îú‚îÄ‚îÄ data_utils.py              # Data preprocessing and vocabulary management
‚îú‚îÄ‚îÄ huffman_tree.py            # Huffman tree implementation for hierarchical softmax
‚îú‚îÄ‚îÄ hierarchical_softmax.py    # Hierarchical softmax training procedures
‚îú‚îÄ‚îÄ negative_sampling.py       # Negative sampling implementation
‚îú‚îÄ‚îÄ cbow_model.py              # CBOW model architecture
‚îú‚îÄ‚îÄ skipgram_model.py          # Skip-gram model architecture
‚îú‚îÄ‚îÄ word2vec_trainer.py        # Main training orchestrator
‚îú‚îÄ‚îÄ evaluation.py              # Comprehensive evaluation utilities
‚îú‚îÄ‚îÄ demo.py                    # Complete demonstration script
‚îú‚îÄ‚îÄ requirements.txt           # Python dependencies
‚îî‚îÄ‚îÄ README.md                  # This file
```

## üõ†Ô∏è Installation

### Prerequisites
- Python 3.7+
- PyTorch 1.10+
- NumPy 1.21+

### Quick Install
```bash
# Clone or download the repository
git clone <repository-url>
cd Word2Vec

# Install dependencies
pip install -r requirements.txt

# Run the demo
python demo.py
```

### Manual Installation
```bash
# Core dependencies
pip install numpy torch pandas scipy scikit-learn matplotlib seaborn

# Optional: GPU support (if CUDA available)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

## üéØ Quick Start

### Basic Usage
```python
from word2vec_trainer import Word2VecTrainer, Word2VecConfig

# Configure training
config = Word2VecConfig()
config.model_type = 'skipgram'              # or 'cbow'
config.training_method = 'negative_sampling' # or 'hierarchical_softmax'
config.embedding_dim = 100
config.num_epochs = 5
config.batch_size = 128

# Train the model
trainer = Word2VecTrainer(config)
trainer.train('your_corpus.txt')

# Use the trained model
similar_words = trainer.find_similar_words('king', top_k=5)
print(f"Words similar to 'king': {similar_words}")
```

### Complete Demo
```bash
# Run the comprehensive demo
python demo.py

# This will:
# 1. Create sample data
# 2. Train both Skip-gram and CBOW models
# 3. Evaluate performance
# 4. Generate visualizations
# 5. Create detailed reports
```

## üìä Model Architectures

### Skip-gram Model
```
Input: Target Word ‚Üí Output: Context Words

Target Word ‚Üí Input Embedding ‚Üí Average ‚Üí Output Layer ‚Üí Context Words
```

### CBOW Model
```
Input: Context Words ‚Üí Output: Target Word

Context Words ‚Üí Input Embeddings ‚Üí Average ‚Üí Output Layer ‚Üí Target Word
```

### Training Methods

#### Hierarchical Softmax
- Uses Huffman tree for efficient probability computation
- Reduces complexity from O(V) to O(log V)
- Better for infrequent words

#### Negative Sampling
- Samples negative examples from noise distribution
- Computationally efficient
- Better for frequent words

## üîß Configuration Options

### Model Parameters
```python
config = Word2VecConfig()

# Model architecture
config.model_type = 'skipgram'              # 'skipgram' or 'cbow'
config.training_method = 'negative_sampling' # 'negative_sampling' or 'hierarchical_softmax'
config.embedding_dim = 100                   # Embedding dimension
config.window_size = 5                       # Context window size
config.min_count = 5                         # Minimum word frequency

# Training parameters
config.num_epochs = 5                        # Number of training epochs
config.batch_size = 128                      # Batch size
config.learning_rate = 0.025                 # Initial learning rate
config.num_negative_samples = 5              # Number of negative samples
config.sample = 1e-3                         # Subsampling threshold

# System parameters
config.device = 'cuda'                       # 'cuda' or 'cpu'
config.log_interval = 1000                   # Logging frequency
config.save_interval = 10000                 # Model saving frequency
```

## üìà Evaluation

### Word Similarity
```python
from evaluation import Word2VecEvaluator

evaluator = Word2VecEvaluator(embeddings, vocab, word_to_id, id_to_word)

# Compute similarity between two words
similarity = evaluator.cosine_similarity('king', 'queen')

# Find similar words
similar_words = evaluator.find_similar_words('computer', top_k=10)
```

### Analogy Tasks
```python
# Solve analogies: king - man + woman = ?
result = evaluator.analogy_task('king', 'man', 'woman', top_k=5)
print(f"king - man + woman = {result}")
```

### Clustering Analysis
```python
# Cluster semantically similar words
words = ['red', 'blue', 'green', 'yellow', 'orange']
clusters = evaluator.cluster_analysis(words, n_clusters=2)
```

### Visualization
```python
# Create t-SNE visualization
words_to_plot = ['king', 'queen', 'man', 'woman', 'computer', 'science']
evaluator.visualize_embeddings(words_to_plot, method='tsne', save_path='embeddings.png')
```

## üìã Evaluation Metrics

### Standard Benchmarks
- **Word Similarity**: Spearman correlation with human judgments
- **Analogy Accuracy**: Percentage of correct analogies
- **Clustering Quality**: Inertia and silhouette scores
- **Coverage**: Percentage of vocabulary covered in evaluations

### Sample Results
```
Word Similarity Evaluation:
  Spearman correlation: 0.6543
  P-value: 0.0001
  Coverage: 85.3%

Analogy Evaluation:
  Accuracy: 42.7%
  Correct: 856/2000
  Coverage: 78.2%
```

## üîç Advanced Features

### Custom Corpus Training
```python
# Prepare your own corpus
from data_utils import DataProcessor, Vocabulary

# Read and preprocess text
sentences = DataProcessor.read_corpus('your_corpus.txt')

# Build vocabulary
vocab = Vocabulary(min_count=5, sample=1e-3)
vocab.build_vocabulary(sentences)

# Train with custom data
trainer = Word2VecTrainer(config)
trainer.prepare_data('your_corpus.txt')
trainer.train()
```

### Model Persistence
```python
# Save trained model
trainer.save_model('my_word2vec_model.pth')
trainer.save_embeddings('my_embeddings.npy')

# Load trained model
new_trainer = Word2VecTrainer(config)
new_trainer.load_model('my_word2vec_model.pth')
```

### GPU Acceleration
```python
# Configure for GPU training
config.device = 'cuda'
config.batch_size = 512  # Larger batch size for GPU

# Training will automatically use GPU if available
trainer = Word2VecTrainer(config)
trainer.train()
```

## üìä Performance Optimization

### Memory Efficiency
- Sparse embeddings for large vocabularies
- Efficient negative sampling tables
- Batch processing for GPU utilization

### Speed Optimizations
- Vectorized operations with NumPy
- PyTorch GPU acceleration
- Optimized Huffman tree implementation
- Efficient data loading and preprocessing

### Scalability
- Configurable batch sizes
- Memory-mapped file support
- Distributed training ready architecture

## üß™ Testing

### Run Tests
```bash
# Test individual components
python data_utils.py
python huffman_tree.py
python negative_sampling.py

# Run comprehensive demo
python demo.py
```

### Validation
```python
# Validate model performance
from evaluation import evaluate_embeddings

results = evaluate_embeddings(
    'embeddings.npy',
    'vocabulary.pkl',
    similarity_file='similarity_dataset.txt',
    analogy_file='analogy_dataset.txt'
)
```

## üî¨ Implementation Details

### Huffman Tree Construction
- Binary tree based on word frequencies
- Efficient path encoding for hierarchical softmax
- O(log V) complexity for probability computation

### Negative Sampling
- Unigram distribution with 3/4 power smoothing
- Efficient sampling table construction
- Configurable number of negative samples

### Subsampling
- Frequent word downsampling
- Probability-based filtering
- Maintains semantic relationships

### Learning Rate Scheduling
- Linear decay over training epochs
- Configurable minimum learning rate
- Adaptive adjustment based on progress

## üìö References

1. Mikolov, T., et al. (2013). "Efficient Estimation of Word Representations in Vector Space"
2. Mikolov, T., et al. (2013). "Distributed Representations of Words and Phrases and their Compositionality"
3. Goldberg, Y., & Levy, O. (2014). "word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method"

## ü§ù Contributing

Contributions are welcome! Please feel free to submit pull requests, report bugs, or suggest features.

### Development Setup
```bash
# Install development dependencies
pip install -r requirements.txt
pip install pytest mypy black

# Run type checking
mypy *.py

# Run tests
pytest
```

## üìù License

This implementation is provided for educational and research purposes. Please cite the original Word2Vec papers when using this code in academic work.

## üéì Educational Use

This implementation is designed to be:
- **Pedagogically Clear**: Well-commented code with detailed explanations
- **Modular**: Each component can be studied independently
- **Comprehensive**: Covers all aspects of the Word2Vec algorithm
- **Production-Ready**: Optimized for real-world applications

Perfect for:
- Learning word embedding algorithms
- Understanding neural language models
- Research in natural language processing
- Building word embedding applications

## üèÜ Acknowledgments

- Original Word2Vec authors: Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean
- PyTorch team for the excellent deep learning framework
- NumPy and SciPy communities for numerical computing tools
- Scikit-learn for machine learning utilities

---

**Happy Word Embedding! üéâ**

For questions, issues, or contributions, please open an issue in the repository. 